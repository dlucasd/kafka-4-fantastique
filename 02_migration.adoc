[%notitle]
== Migrer vers 4.x

[.step.without-bullets]
* ğŸ“˜ link:https://kafka.apache.org/documentation/#upgrade_4_0_1_from[Migrer vers 4.x]
* ğŸ§¹ Grand mÃ©nage
* ğŸ’¸ CoÃ»ts et complexitÃ© Ã  maintenir

[.notes]
--
* avant de profiter des fonctionnalitÃ©s, il faut migrer
* documentation mise Ã  jour (il y a eu une KIP lÃ  dessus)
* 1Ã¨re chose Ã  retenir : il y a eu du mÃ©nage
* stoper le support des anciennes versions permet d'*encourager les utilisateurs Ã  migrer plus souvent*
--

[%notitle]
=== Java et Scala


[.step.without-bullets]
* âŒ Java 8 et Scala 2.12 plus supportÃ©s
* â˜•ï¸ Kafka Clients & Kafka Streams â†’ Java 11
* â˜•ï¸ Broker, Connect, Tools, MM2 â†’ Java 17

[.notes]
--
* RÃ©duire le coÃ»t de maintenance multi-JDK
* KIP-720 : MirrorMaker 1 supprimÃ©
* KIP-750 : java
* KIP-751 : scala
* KIP-1032 : jakarta / javaEE
--

=== Protocole client-broker

[.step.without-bullets]
* âŒ Retrait des anciennes API client-broker
* âš™ï¸ Broker 4.0 â‡„ Clients â‰¥ 2.1, Clients 4.0 â‡„ Brokers â‰¥ 2.1
* ğŸ‘¨â€ğŸ’» Ops : activer la mÃ©trique `DeprecatedRequestsPerSec` sur les brokers (3.7+), si >0 alors impact

[.notes]
--
* Il s'agit du **client-broker protocol** (RPC)
* 2.1 sorti en 2018
* KIP-896
--


=== Format de message

[.step.without-bullets]
* âŒ Formats de message v0/v1 plus supportÃ©s en Ã©criture
* âœ‰ï¸ Le â€œnouveauâ€ format v2 devient l'unique format cÃ´tÃ© Ã©criture

[.notes]
--
* "nouveau" format v2 introduit avec Kafka 0.11.0.0 (juin 2017)
* Objectifs: cohÃ©rence, performances (moins de conversions), et bÃ©nÃ©ficier nativement d'idempotence, transactions, headers, leader epoch, etc. (tous liÃ©s Ã  v2).
* KIP-724
--

=== Migration vers Log4j2

[.step.without-bullets]
* âŒ Sortir dÃ©finitivement de Log4j 1.x
* âš™ï¸ Outil de migration `log4j-transform-cli`
* ğŸ’» `config-file convert -i log4j1 -o log4j2 <inputFile> <outputFile>`


[.notes]
--
* Log4j 1.x (obsolÃ¨te depuis 2012)
* pas obligÃ© de migrer mais l'Ã©quipe log4j2 annonce une *compatibilitÃ© limitÃ©e*
* hors scope : l'appender log4j1 fournit pour ne pas casser les configs existantes
* KIP-653
--

=== Config par dÃ©faut modifiÃ©e

[.step.without-bullets]
* â±ï¸ **linger.ms**: 0 ms â†’ **5 ms** (batching â†‘, latence +~5 ms)
* â±ï¸ **(log.)message.timestamp.after.max.ms**: âˆ â†’ **1 h** (protÃ¨ge des timestamps â€œfutursâ€)

[.notes]
--
* linger.ms: batching par dÃ©faut plus efficace. le batch.size permet toujours de dÃ©clencher l'envoi.
* after.max: Ã©viter rotations anormaux si des timestamps â€œfutursâ€ sont envoyÃ©s par erreur
* KIP-1030
--

[%notitle]
=== Config par dÃ©faut modifiÃ©e

[.step.without-bullets]
* ğŸ“ **(log.)segment.bytes (min)**: 14 B â†’ **1 MB** (Ã©vite micro-segments)
* **num.recovery.threads.per.data.dir**: 1 â†’ **2** (rÃ©tablissement plus rapide)

[.notes]
--
* segment.bytes: attention c'est le min, pas le default
* recovery.threads: *rÃ©tablissement* plus rapide aprÃ¨s arrÃªt non propre, il faut adapter au nombre de coeurs
* d'autres configs Ã  aller voir
* KIP-1030
--


[%notitle%auto-animate]
=== Zookeeper

image:images/zookeeper.png[]

[%notitle%auto-animate.columns.is-vcentered]
=== Zookeeper

[.column.is-one-third]
--
image:images/zookeeper.png[]
--

[.column.has-text-right]
****
[.step.without-bullets]
* ğŸ—³ï¸ Consensus distribuÃ©, Ã©lection de leader
* ğŸ¤ Coordination du cluster (Broker en vie ? Qui est controller ? Pannes, dÃ©couvertes ...)
* ğŸ’¾ Stockage des mÃ©tadonnÃ©es (topics, partitions, ACL, ...)
****

[.notes]
--
* broker alive, qui est le controller
--

=== Avec ZooKeeper

image:images/zooKeeperCluster.svg[width=50%]

[.notes]
--
* limitations 200 000 partitions par cluster
* ralentissement sur la lecture des mÃ©tadata
* ajout de complexitÃ© pour les ops : deux systÃ¨mes Ã  maintenir
--


=== KRaft

[.notes]
--
* production ready depuis la 3.5
* vient de Raft : algorithme de consensus
--

=== Avec KRaft

image:images/KRaft.svg[width=60%]


[.notes]
--
* stockage des *metadata sur un topic interne*
* *rÃ©duit la latence* des opÃ©rations de gestion du cluster
* amÃ©liore la *scalabilitÃ©* et la *rÃ©silience*
* simplifie le dÃ©ploiement et l'exploitation : 1 seul processus
* un broker est soit un broker normal, soit un controller, soit les 2 (mode combinÃ©)
* en prod dÃ©ployer 3 controllers au moins pour de la HA
--

=== Migration

[.step.without-bullets]
* ğŸ¥ Avoir un cluster sain (pas de partition offline, rÃ©plication stable ...)
* â¬†ï¸ Monter en 3.9.x
* ğŸ§ª Migration en environnement de tests
* (et tester la procÃ©dure de rollback!)
* ğŸ“ˆ Monitorer les logs et JMX

[.notes]
--
* mode bridge dispo depuis la 3.5
* rolling update sans downtime
--
