== Bonus


=== Topology

image::images/topology.svg[opts=inline, width=70%]


[.notes]
--
* problème: il faut créer un processor custom et l'injecter manuellement dans toutes les topologies
* donne un “hook” officiel et propre pour injecter une couche transversale (log/metrics/tracing/tests) sur tous tes processors Streams (DSL/Processor API) via un ProcessorWrapper configurable au build de la topologie
* KIP-1112
--

=== Implémentation exemple

[.large-code-exemple]
--

[source,java,highlight=1..2|6..7|11..14|28..31|16..26|37..39]
----
public class LoggingProcessorWrapper<KIn, VIn, KOut, VOut> 
  implements ProcessorWrapper<KIn, VIn, KOut, VOut> {

...

    private static class LoggingProcessor<KIn, VIn, KOut, VOut> 
       implements Processor<KIn, VIn, KOut, VOut>, ProcessorContextAware {

        ...

        @Override
        public void init(ProcessorContext<KOut, VOut> context) {
            delegate.init(context);
        }

        @Override
        public void process(Record<KIn, VIn> record) {
            Instant start = Instant.now();
            logger.info("Entrée dans le processeur");

            delegate.process(record);

            var duration = Duration.between(start, Instant.now());
            logger.info("Sortie du processeur : durée={}", duration.toMillis());
        }

        @Override
        public void close() {
            delegate.close();
        }
    }
}



________________________________
application.properties:

processor.wrapper.class=com.mirakl.LoggingProcessorWrapper


----

--

[.notes]
--
* introduit une interface ProcessorWrapper
* wrapper au même titre que pattern delegate ou decorator
* cas d'usage :
** audit, logging, métriques, tracing
** gestion d'erreurs/retry (ex: dead-letter queue)
** possibilité de faire du feature flag
* test : permet de simuler des erreurs pour valider le comportement de l'application, chaos wrapper
* KIP-1112 - 4.0.0
--


=== KStreams Foreign Key Extractor

Avant :


[.large-code-exemple]
--

[source,java]
----
// Topic orders, key : {clientId, orderId}, value : {productId, quantity}
// Topic clients, key : {clientId}, value : {name, address}

// Étape intermédiaire : Ajouter orderId à la valeur
KTable<OrderKey, OrderWithClient> ordersWithClient = orders.mapValues(
    (key, order) -> new OrderWithClient(key.getClientId(), order.getProductId(), order.getQuantity()),
);

KTable<OrderKey, EnrichedOrder> enriched = ordersWithClient.join(
    clients,
    // Extracteur de clé étrangère : uniquement la valeur
    orderWithClient -> new ClientKey(orderWithClient.getClientId()),
    (orderWithClient, client) -> EnrichedOrder.of(orderWithClient.getProductId(), orderWithClient.getQuantity(), client.getName(), client.getAddress()),
);
----

--

[%notitle]
=== Après

Après :


[.large-code-exemple]
--

[source,java]
----
KTable<OrderKey, EnrichedOrder> enriched = orders.join(
    clients,
    // Extracteur de clé étrangère : utilise clé (OrderKey) et valeur (Order)
    (orderKey, order) -> new ClientKey(orderKey.getClientId()),
    (order, client) -> EnrichedOrder.of(order.getProductId(), order.getQuantity(), client.getName(), client.getAddress()),
);
----

--

[.notes]
--
* ajoute une surcharge de KTable.join (et leftJoin...)
* KIP-1104 - 4.0.0
--


=== KRaft architecture

image:images/zoomKRaft.svg[width=70%]

[.notes]
--
* métadata stockées dans un topic Kafka interne : 1 partition
* cache et offset pour éviter de relire tout le log à chaque démarrage
* event sourcing : persiste tous les opérations menant à l'état courant
* meilleure performance : de l'ordre de secondes à démarrer, au lieu de minutes côté zookeeper
--

=== Protection contre les deadlocks dans le Producer

* *Problème*: `flush()` dans callback de `send()` → Deadlock sur ioThread
** Timeout confusant, app bloquée, diagnostic difficile
* *Solution*: `KafkaException` immédiate

[.notes]
--
* Le callback de Producer.send() est exécuté par le thread réseau du producteur (ioThread).
* Si, dans ce callback, le code appelle producer.flush(), on crée une attente circulaire:
** flush() attend que le thread réseau vide les buffers,
** mais le thread réseau est justement occupé à exécuter le callback.
* KIP-1118 - 4.1.0
--


=== Amélioration gestion d'erreurs transactionnelles dans le Producer

* *Problème*: Exceptions floues pour transactions (Retriable? Fatal? Abortable?)

[.notes]
--
* rappel : Transactions Kafka pour read-process-write atomique ou write dans au moins deux topics
* KIP-1050 - 4.1.0
--

[%notitle]
=== Solution

* *Solution*: 4 catégories + hiérarchie claire
** **Retriable** (ex: TimeoutException) : Retry auto
*** **RefreshRetriable** (ex: UnknownTopic) : Refresh metadata + retry
** **Abortable** : Abort transaction (ex: CommitFailed)
** **ApplicationRecoverable** : Restart app/producer (ex: ProducerFenced)
** **InvalidConfig** : Fix config (ex: AuthError)


[.notes]
--
* RefreshRetriable : Le client rafraîchit les métadonnées puis réessaie
--


[%notitle]
=== Outils de gestion des groupes

image:images/groupes.svg[width=80%]

[.notes]
--
* Aujourd'hui, “group” ne veut pas dire seulement “consumer group classique”
* KIP-1043
--



=== Outils de gestion des groupes


=== Nouvel outil kafka-groups.sh

[.large-code-exemple]
--

[source,bash]
----
$ bin/kafka-groups.sh --bootstrap-server localhost:9092 --list
GROUP                   TYPE          PROTOCOL
old-consumer-group      Classic       consumer
new-consumer-group      Consumer      consumer
connect-cluster         Classic       connect
share-group             Share         share
schema-registry         Classic       sr
simple-consumer-group   Classic
----

--

[.notes]
--
* Aujourd'hui, “group” ne veut pas dire seulement “consumer group classique”
* un nouvel outil kafka-groups.sh pour lister tous les types de groupes
* des API Admin plus nettes (listGroups universel, describeClassicGroups)
* KIP-1043
--

=== Adaptation des outils existants

* kafka-consumer-groups.sh
* kafka-share-groups.sh

[.notes]
--
* Enrichir les outils d'admin (kafka-consumer-groups.sh, kafka-share-groups.sh) 
* Enrichir AdminClient pour exposer les nouveaux indicateurs introduits par le protocole de groupe KIP-848:
** Statut de migration des membres “classic → consumer” (flag upgraded)
* KIP-1099
--